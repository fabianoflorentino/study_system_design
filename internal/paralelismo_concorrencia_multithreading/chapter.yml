---
title: Paralelismo, Concorrência e Multithreading
url: https://fidelissauro.dev/concorrencia-paralelismo/
topics:
  - name: Introdução
    content: |
      Este artigo é o primeiro de uma série sobre system design. Esta série tem como objetivo explicar conceitos complexos
      de programação de maneira simples e objetiva para todos os tipos de profissionais, independentemente do nível de
      senioridade ou tempo de experiência, contribuindo para a fixação de conceitos de ciência da computação e arquitetura.

      Comecei a escrever esses textos em 2021, quando tinha a intenção de produzir material explicativo sobre engenharia
      para profissionais de Site Reliability Engineering (SRE). Hoje, revendo com uma nova perspectiva, consegui revisar
      esse material para torná-lo útil e acessível a todos.

      Todos os artigos vão, em algum momento, utilizar analogias como "mundo real" para tornar a lógica mais clar e facilitar
      a explicação e compreensão. Neste texto, vou explicar tópicos como Multithreading, Concorrência e Paralelismo.

      Não é meu objetivo detalhar exaustivamente todos os aspectos do mundo ou explicar todos os tópicos envolvendo esse
      tema com termos complexos da literatura. Meu objetivo é que você compreenda os conceitos, consiga aplicar e,
      principalemente, explicar para outra pessoa usando os mesmos exemplos ou criando novos. Prometo tornar isso divertido

      Utilizaremos a linguagem de programação Go para exemplificar alguns algoritmos. Embora utilizaremos recursos nativos
      como Goroutines, Channels e WaitGroups, a ideia não é tornar este material um artigo específico sobre a linguagem;
      Ele pode ser aproveitado conceitualmente para diversos contextos.

      Vamos começar detalhando alguns conceitos que serão úteis durante o artigo:

  - name: O que é um Processo?
    content: |
      Um processo é basicamente uma instância de um programa em execução. Esse programa contém uma série de instruções, e
      o processo é a  execução real dessas instruções. Em outras palavras, um processo é um programa em ação.

      Ao iniciarmos aplicativos como o navegador, IDE, agentes, aplicações, banco de dados e outros serviços, o sistema
      operacional cria um processo para cada um desses programas, fornecendo os recursos necessários para sua execução.
      Isso inclui espaço de memória isolado, threads, contextos e a gestão do próprio ciclo de vida do processo.

  - name: O que é uma Thread?
    content: |
      Uma thread é a menor unidade de processamento que pode ser gerenciada por um sistema operacional. Ele representa uma
      sequência de instruções programadas que pode ser executadas de forma independente em um núcluo de CPU. Dentro do
      processo, múltiplas threads podem ser utilizadas para realizar tarefas de forma concorrente, visando melhorar a
      eficiência do programa, enquanto uma thread aguarda por uma ação demorada, como uma requisição HTTP, o programa pode
      prosseguir com a execução de outras threads. As threads de um mesmo programa compartilham o mesmo espaço de memória
      e os recursos alocados. Sistemas que possuem múltiplas CPUs, ou CPUs com múltiplos núcleos, podem executar threads
      simultaneamente em núcleos diferentes da CPU, permitindo o paralelismo. Imagine as threads como várias tarefas menores
      que precisam ser realizadas em um churrasco.

  - name: O que é Multithreading?
    content: |
      Multithreading é uma técnica de programação que consiste na criação de múltiplas threads (fluxos de execução independente)
      dentro de um único processo. Cada thread pode ser responsável por diferentes tarefas ou partes de uma tarefa mais ampla.
      Este método pode ser aplicado tanto em contextos concorrentes quanto paralelos. Em sistemas com um único núcleo do
      processador, o multithreading facilita a concorrência (alternância rápida entre threads para criar a ilusão de simultaneamente).
      Já em sistemas multiprocessadores, ou com múltiplos núcleos, o multithreading pode alcançar paralelismo real, com
      threads sendo executadas simultaneamente em núcleos distintos da CPU**, otimizando o uso dos recursos e melhorando
      a eficiência e o desempenho do processo.

      Para ilustrar o conceito de multithread, pense em seus restaurantes favoritos. Aqui, o processo é o restaurante
      funcionando, com o objetivo de servir comida aos clientes. Durante um horário de pico, como o almoço em um dia útil,
      as threads são como os funcionários da cozinha. Cada cozinheiro (thread) é responsável por preparar um prato diferentes
      simultaneamente, acelerando o atendimento dos pedidos. Dessa forma, vários pratos são preparados ao mesmo tempo,
      aumentando a eficiência e diminuindo o tempo de espera dos clientes.

      Agora que já exploramos alguns conceitos teóricos importantes, podemos seguir com a explicação mais detalhada.

  - name: Concorrência
    content: |
      Imagine que você esta preparando um churrasco sozinho. Você é responsável por organizar a geladeira, fazer os cortes
      de carne, preparar os vegetais para os amigos vegetarianos, fazer caipirinhas e gelar a cerveja. Você alterna entre
      essas tarefas, trabalhando um pouco em cada uma, apesar de ser responsável por todas elas.

      Esse cenário é um exemplo de concorrência. onde você está gerenciando várias tarefas, mas não necessariamente trabalhando
      em mais de uma delas simultaneamente. Você se alterna entre as tarefas, criando a impressão de que tudo está progredindo
      ao mesmo tempo.

      Concorrência é sobre lidar com muitas tarefas ao mesmo tempo, mas não de forma simultânea. É a habilidade de uma
      aplicação gerenciar múltiplas tarefas e instruções em segundo plano, mesmo que essas instruções não sejam sendo
      processadas ao mesmo tempo, ou executadas em outros núcleos do processador.

  - name: Exemplo de Implementação
    Content: |
      Agora, vamos criar um algoritmo que abstrai o nosso churrasco. Este algoritmo seguirá a lógica:
      - Listar as atividades do churrasco.
      - Executar essas tarefas em goroutines simultâneas, com cada uma aguardando seu respectivo tempo de preparo.
      - Monitorar a conclusão das atividades.

      Link: https://go.dev/play/p/d7HzIKIRnD0

  - name: Paralelismo
    content: |
      Ainda estamos no exemplo do churrasco. Desta vez você tem amigos para ajudar: um corta a carne, outra acende
      churrasqueira, outro gela a cerveja e mais um faz a caipirinha. Todas essas tarefas estão ocorrendo em paralelo,
      com cada pessoa responsável por uma parte do processo.

      Isso ilustra o paralelismo. Mútiplas tarefas e instruções ocorrendo simultaneamente, executadas por múltiplos núcleos
      de processadores.

      Diferentemente da concorrência, onde se trata de gerenciar várias tarefas ao mesmo tempo, mas com a apenas uma ativa
      por vez, o paralelismo envolve fazer várias coisas ao mesmo tempo.

      Paralelismo é empregado em situações onde o desempenho e a eficiência são críticos, e há recursos suficientes, como
      múltiplos núcleos de CPU, para executar diversas tarefas simultaneamente.

      Em ambientes paralelos, processos ou threads frequentemente precisam coordenar suas ações, e comunicar-se entre si.
      Mecanismos de sincronização, como semáforos, mutexes e monitores, são ferramentas essenciais para evitar race
      conditions e garantir a consistência dos dados, embora isso possa acrescentar complexidade à programação e ao debugging
      de programas que implementam paralelismo.

      Paralelismo em computação é um campo de pesquisa ativo e continua evolindo, especialmente com o desenvolvimento de
      novas arquiteturas de hardware e a crescente demanda por processamento de grandes volumes de dados e computação de
      alto desempenho.

  - name: Implementando um algoritmo de paralelismo
    content: |
      Vamos simular novamente um churrasco em código, mas agora sob condições de paralelismo. Neste snippet, vamos:
      - Identificar quantos amigos (CPUs) estão disponíveis para ajudar no churrasco.
      - Criar uma lista de atividades do churrasco, informando o tempo de preparo e quem é o responsável por cada tarefa.
      - Determinar o número ideal de tarefas e distribuí-las entre os amisog de forma equilibrada.
      - Alocar as tarefas entre os amigos (CPUs) em threads.
      - Monitorar o output das tarefas.

      Link: https://go.dev/play/p/2qEtDrT9p2V

  - name: Paralelismo Externo vs Paralelismo Interno
    content: |
      O paralelismo pode ser divido em duas categorias: interno e externo.

    subtopics:
      - name: Paralelismo interno
        content: |
          O paralelismo interno, também conhecido como paralelismo intrínceso, ocorre dentro de um
          processo. É o paralelismo que você implementa no código da sua aplicação quando precisa dividir tarefas ou itens em
          memória entre várias sub-tarefas que podem ser processadas simultaneamente. Basicamente, é o paralelismo que você
          cria via código para ser executado dentro do seu container ou servidor.

      - name: Paralelismo Externo
        content: |
          Já o paralelismo externo refere-se à execução simultânea de múltiplas tarefas em diferentes
          hardwares, máquinas ou containers. Esse conceito é aplicado em ambientes de computação distribuída, como Hadoop e
          Spark, consumo de mensagens vindas de messages brokers como RabbitMQ, SQS, streamings como Kafka que distribuem
          granges volumes de dados em vários servidores e instâncias para realizar tarefas de ETL, Machine Learning e Analytics.
          Também é visto em Load Balancers, que dividem as requisições entre várias instâncias da mesma aplicação para distribuir
          o tráfego.

          Link: https://lucid.app/lucidspark/d7538443-6d16-4b4e-a591-b1e8b8f5ff67/edit?viewport_loc=-11%2C-11%2C2560%2C1277%2C0_0&invitationId=inv_e7c2a3b6-113c-4757-9fb4-c4f357c78cc7

  - name: Paralelismo vs Concorrência
    content: |
      Após uma análise detalhada, conseguimos distinguir conceitualmente concorrência de paralelismo. A concorrência lida com
      a execução de várias tarefas ao mesmo tempo, permitindo que um sistema execute múltiplas operações aparentemente
      simultâneas. Já o paralelismo envolve a execução literal de várias operações ou tarefas ao mesmo tempo.

      Concorrência no mais, significa também ter várias tarefas em parelelo onde você não tem controle na ordem que elas
      serão processadas, tendo em vista que só é possível saber a ordem de execução após todas elas terem terminado.

      Em sistemas com um único núcleo de CPU, a concorrência é normalmente alcançada através de multithreading, onde as
      tarefas são alternadas rapidamente, criando a ilusão de execução simultânea. Por outro lado, o paralelismo requer
      hardware com múltiplos núcleos, permitindo que cada núcleo execute diferentes threads ou processos simultaneamente.

      Paralelimos em geral é concorrência, mas nem toda concorrência é paralela.

      Link: https://lucid.app/lucidchart/d9a7c2f1-292a-46c7-aab5-61543328cd13/edit?invitationId=inv_5eb2f68d-cbcd-453d-bb9d-57cd170e8d6a

  - name: Lidando com Parelismo e concorrência
    content: |
      Agora que detalhamos de forma lúdica e conceitual a definição de programação paralela e concorrente, é hora de explorar
      os desafios e ferramentas existentes para trabalhar com essas estratégias. Embora abordagens paralelas e concorrentes
      ofereçam várias vantagens, como melhoria de performance, escalabilidade e otimização de recursos, elas também trazem
      desafios significativos. Estes incluem questões de coordenação, condições de corrida, deadlocks, starvation, balanceamento
      de carga de trabalho entre outros. Vamos agora definir conceitualmente alguns desses termos para facilitar seu
      entendimento e capacidade de explicá-los no futuro.

    subtopics:
      - name: Deadlocks e Starvation
        content: |
          Image que você está usando a grelha, um recurso compartilhado no seu churrasco, para preparar o pão de alho,
          enquanto seu amigo está segurando a espátula (lembre-se, nada de furar a carne com garfo, pelo a mo de Deus...),
          outro recurso essencial. Você precisa de espátula, e seu amigo precisa da grelha. Ambos aguardam que o recurso
          do outro fique disponível, mas nenhum de vocês libera o recurso que possui, criando um impasse onde nenhuma das
          tarefas pode prosseguir. Isso exemplifica um Deadlock.

          Um deadlock ocorre quando duas ou mais threads (ou processo) entram em um estado de espera permanente, pois cada
          uma está esperando por um recurso que está sob pose de outra. Em suma, existe um ciclo de dependência que impede
          qualquer avanço.

          Agora, imagine outra situação onde cada um precisa preparar sua própria refeição devido a preferências específicas
          de ponto da carne. Seu grupo de amigos se divide entre os mais ágeis, cara-de-pau e esfomeados, e os mais educados
          e lentos. À medida que as grelhas ficam disponíveis, o primeiro grupo ocupa rapidamente os espaços, deixando pouco
          ou nenhum acesso para o segundo grupo. Como resultado, as pessoas mais tranquilas enfretam dificuldades para assar
          sua comida, experimentando um espécie de Starvation.

          Starvation, ou inanição, ocorre quando uma ou mais threads não conseguem acessar os recursos necessários por um
          longo período. Isso é frequentemente causado por uma alocação de recurso desigual, onde certas threads são
          priorizadas em detrimento de outras.

      - name: Race Conditions (Condições de Corrida)
        content: |
          Imagine que você está organizando outro churrasco com seus amigos. Desta vez há apenas uma churrasqueira disponível
          para grelhar todos os alimentos. Você precisam preparar picanha, maminha, legumes, abacaxi, linguiça, pão de alho
          e mais. A churrasqueira é pequena e só permite assar um tipo de alimento por vez. Aqui, a churrasqueira representa
          um recurso compartilhado, e uma Race Condition (condição de corrida) pode surgir se todos os alimentos forem
          preparados para assar simultaneamente.

          Race condition é um fenômeno comum quando um recurso compartilhado é acessado e modificado por várias tarefas ou
          threads em parelelo. O estado final desse recurso pode depender da ordem em que as modificações são realizadas,
          que pode variar a cada execução. Por exemplo, considere o seguinte algoritmo:

            - Inicialmente, temos um número definido de itens (como 100) disponíveis para serem grelhados.
            - Temos um contador que registra o número de itens que forem grelhados.
            - Idealmente, se começarmos com 100 itens disponíveis, o contador de itens grelhados também deveria terminar
            com o valor 100.

          Podemos observar que o resultado varia de acordo com a ordem e o tempo que as goroutines acessam o contador.
          Esse é o cerne do problema de race condition: a inconsistência dos resultados devido ao acesso simultâneo ao
          mesmo recurso. Emboraa analogia com o churrasco seja útil para ilustrar o conceito, na realidade prática de um
          churrasco, a grelha não comportaria todos os alimentos ao mesmo tempo. Portanto, a situação de race condition,
          neste contexto, seria menos provável, já que a limitação física da grelha impõe um controle natural sobre o
          acesso simultâneo. No entando, em sistema de computação, onde múltipas threads podem acessar e modificar o mesmo
          recurso sem uma devida sincronização, a race condition se torna um problema significativo e desafiador.

          Link: https://go.dev/play/p/QQQwp9YuikV

  - name: Mutex
    content: |
      Voltando ao churrasco, após observar o exemplo de Race Conditions, concluímos que é impossível assar todos os alimentos
      ao mesmo tempo na grelha, já que ela só comporta um item por vez. Portanto, é necessário que alguém do churrasco fique
      responsável por assas os alimentos em sequência.

      Como a churrasqueira é um recurso compartilhado, essa pessoa atuará como uma "trava" para o uso da churrasqueira:
      um alimento é assado, e então o próximo é colocado. Esse cenário ilustra a função de um Mutex.

      Mutex é a abreviação de Mutual Exclusion (Exclusão mútua), e é uma estratégia eficiente para controlar o acesso a
      um recurso compartilhado em ambientes de multithreading, paralelismo ou concorrência. Ela possibilita um acesso
      sequencial e organizado aos recursos, sendo uma das principais ferramentas para programação concorrente e paralela.

      O principal objetivo do Mutex é evitar race conditions, como visto anteriormente, garantindo que apenas uma thread
      por vez possa acessar um recurso. As operações básicas de um Mutex são Lock, para bloquear o acesso ao recurso, e
      unlock para liberá-lo para a próxima thread.

      Estas operações de lock/unlock também deve respeitar uma certa prioridade, ou seja, apenas a thread que bloqueou o
      recurso pode desbloqueá-lo.

      Sem um mutex, todos tentariam usar a churrasqueira simultaneamente, resultando em confusão, dispurtas e, possivelmente,
      alimentos mal preparados ou queimados.

      Contudo, o uso de mutexes não está isento de riscos, sendo o principal deles o Deadlock. Um deadlock ocorre quando
      várias threads tentam bloquear múltiplos mutexes em uma ordem inconsistente.

      No Go, podemos trabalhar com Mutexes através do pacote sync. Para solucionar o problema de race conditio com a grelha,
      podemos:
        - Criar um orquestrador para o uso da grelha, chamado de grelhaOcupada, usando o sync.Mutex.
        - Durante a preparação, na funcão grelhar(), inserimos um Mutex.Lock() no início e um Mutex.Unlock() no final para
        liberar o recurso para a próxima thread.
        - Assim, garantimos um acesso sequencial a todos os processos para grelhar itens na churrasqueira.

      Link: https://go.dev/play/p/sjqz6rD_aYB

  - name: Mutex Distribuído
    content: |
      Já exploramos o uso de Mutex no modelo de paralelismo interno, onde o controle de paralelismo é implementado via
      código. É igualmente importante entender a aplicação dessa lógica no paralelismo externo, em cenários arquiteturais
      diversos como o consumo de mensagens de uma fila, eventos de um tópico do kafka, tratamento de solicitações HTTP e
      outras situações que demandam idempotência, atomicidade e exclusividade em determinados processos.

      Desenvolver um Mutex para sistemas distribuídos apresenta uma série de desafios, mas em alguns aspectos, é mais
      facilitado do que os Mutexes em cenários de paralelismo interno com memória compartilhada. Entre os possíveis
      problemas que podemos encontrar estão a comunicação entre componentes, latência de rede e falha gerais nos serviços.

      Para funcionar eficientemente, esses sistemas geralmente dependem de uma base de dados centralizada para manter o
      estado dos processos compartilhados entre todas as réplicas dos consumidores de mensagens. Isso é crucial para lidar
      com duplicidade de mensagens, eventos ou solicitações devido a cenários imprevistos.

      Algumas estratégias comuns utilizam banco de dados otimizados para operações de leitura e escrita chave/valor, como
      Redis, Memcached, Cassandra, DynamoDB, além de tecnologias como Zookeeper.

      No exemplo a seguir, que utilizamos o Redis para apresentar um fluxo lógico de um algoritmo de Mutex. Ao receber um
      pseudo-mensagem, verificamos se já existe um lock para ela no Redis. Se o lock existir, descartamos o processamento
      da mensagem. Se não existir, criamos o lock, processamos a menssagem e, em seguida, liberamos o lock.

    subtopics:
      - name: Mutex Distribuído - Zookeeper
        content: |
          Uma alternativa elegante ao Redis para gerenciar locks distribuídos é o uso do Apache Zookeeper. Embora a lógica
          fundamental seja semelhante ao exemplo anterior, o Zookeeper apresenta algumas peculiaridades interessantes.

          A criação de um mutex distribuído com Go utilizando Apache Zookeeper é uma tarefa avançada que exige a manipulação
          de znodes, (nós do Zookeeper), para gerenciar as travas de forma distribuída. Vamos explorar um exemplo básico
          de como isso pode ser implementado.

          Uma vantagem notável dos locks do Zookeeper é a capacidade de definir um timeout de sessão. Isso garante que
          todos os locks gerenciados pelo processo sejam excluídos automaticamente após o término de execução do programa.

          Segue a lógica para o uso do Zookeeper na gestão de locks:

          - Verificação de lock: Checar se já existe um lock para o recurso específico.
          - Gerenciamento de Mensagem: Caso o lock exista, a mensagem é ignorada e retornada ao pool. Se não existir,
          prosseguir para o próximo passo.
          - Criação de lock: Estabelecer um lock para o recurso.
          - Processamento da Solicitação: Realizar as operações necessárias enquanto o lock está ativo.
          - Remoção do lock: Após o processamento bem-sucedido, remover o lock para liberar os recursos.

          Link: https://github.com/fabianoflorentino/study_system_design/blob/main/internal/paralelismo_concorrencia_multithreading/zookeeper.go
          Infra utilizada: https://github.com/fabianoflorentino/study_system_design/blob/main/docker-compose.yml

  - name: Spinlock
    content: |
      Imagine novamente o churrasco com seus amigos, onde há uma única grelha que todos desejam utilizar para assar diferentes
      alimentos. Diferentemente do Mutex, onde se espera pacientemente pela liberação do recurso, no caso do Spinlock, cada
      pessoa permanece ao lado da grelha, verificando constantemente se ela está livre. Assim que a grelha fica disponível,
      a pessoa que está naquele exato momento a utiliza.

      Um Spinlock é um mecanismo de sincronização utilizado em ambientes de programação concorrente para proteger o acesso
      a recursos compartilhados. A ideia por trás de um spinlock é relativamente simples: um vez de bloquear uma thread
      e fazê-la entra em estado de espera (sleep) quando tenta acessar um recurso já bloqueado, a thread contina ativa
      (girando) em um loop até que o lock seja liberado.

      Esta abordagem de "girar" em um loop, constantemente verificando se o recurso está disponível, é eficaz em cenários
      onde o tempo de espera pelo recurso é relativamente curto, pois evita o overhead associado ao bloqueio e desbloqueio
      de threads. No entando, em situações onde o recurso permanece bloqueado por períodos mais longos, o spinlock pode
      ser meno eficiente, pois a thread continua consumindo recursos de CPU enquanto "girar".

      Pense em um spinlock como uma situação em um churrasco onde, em vez de formar uma fila e aguardar a sua vez de usar
      a grelha (o que seria um bloqueio tradicional como vimos no Mutex), cada pessoa fica parada ao lado da grelha
      perguntando toda hora se ela está livre. Assim que a grelha é liberada, a pessoa que verificar naquele momento a
      utiliza. Esta abordagem é eficiente se o tempo de espera pela grelha for curto, mas pode ser cansativa e ineficiente
      se a grelha estiver ocupada por longos períodos.

    subtopics:
      - name: Exemplo de Implementação
        content: |

          Link: https://go.dev/play/p/AsoJtOIUyde

  - name: Semáforos e Worker Pools
    content: |
      Existem dois tipos principais de semáforos: o Semáforo Binário, que similar ao Mutex já discutido, e o Semáforo Contador,
      que vamos abordar agora.

      Imagine um churrasco com uma grelha maior, capaz de comportar um número definido de alimentos simultaneamente. A grelha
      representa um recurso compartilhado, e a capacidade máxima de alimentos que ela pode assar por vez ilustra o conceito
      de um semáforo contador.

      Suponhar que a grelha pode acomodar até 3 pedaços de carne de cada vez. Cada alimento colocado na grelha ocupa um
      espaço do semáforo, decretementando seu valor até atingir 0, indicando que a grelha está completamente ocupada. Quando
      um alimento é retirado da grelha, o contador é incrementado, indicando que há espaço mais um alimento se assado.

      Um semáforo é outro mecanismo de sincronização usado em programação paralela para controlar o acesso a recursos
      compartilhados e evitar Race Conditions e inconsistências de dados. Ele se baseia em operações atômicas, que incluem:

        - Wait (Ocupar um recurso): Utilizada para adquirir um recurso. Por exemplo, em um semáforo com 3 posições, podemos
        ter no máximo 3 threads trabalhando simultaneamente. Ao executar Wait(), o número disponível é decrementado, indicando
        uma posição está ocupada.

        - Signal (Liberar um recurso): O oposto do Wait, a operação Signal() incrementa o contador de semáforo até o limite
        especificado. Quando um processo em Wait() conclui, ele chama Signal() para liberar um espaço permitindo que outra
        thread ocupe esse lugar.

      Os semáforos são eficientes para trabalhar com Worker Pools, que são conjuntos de threads executando tarefas de forma
      controlada. Esse padrão é útil quando há muitas tarefas a serem realizdas, mas é necessário limitar o número de threads
      em execução simultânea. Na nossa analogia, o Worker Pool seria o número de alimentos que a grelha pode acomodar.

    subtopics:
      - name: Exemplo de Implementação
        content: |
          - Determinar a capacidade da grelha: 3
          - Verificar a quantidade de comida disponível para o churrasco: 10
          - Criar um channel com o tamanho da capacidade da grelha para gerenciar o uso.
          - Iniciar o preparo da comida, ocupando um espaço no semáforo ao começar a assar o alimentos.
          - Remover um espaço de semáforo a lógica de incrementar / decrementar. Vamos criar um canal com o tamanho máximo
          de itens que cabem na grelha, adicionar um objeto para ocupar a posição e em seguida removê-lo quando liberar o
          processo.

          Link: https://go.dev/play/p/qZmrpyU_6a9

  - name: Resumo
    content: |
      Neste artigo, exploramos os conceitos de Multithreading, Concorrência e Paralelismo, e como eles podem ser aplicados
      em sistemas de computação. Através de analogias lúdicas e exemplos práticos, conseguimos ilustrar a diferença entre
      concorrência e paralelismo, e como esses conceitos são fundamentais para a programação eficiente e escalável.

      A programação paralela e concorrente é uma área complexa e desafiadora, mas essencial para o desenvolvimento de
      sistemas modernos e eficientes. Através de técnicas como Mutexes, Spinlocks, Semáforos e Worker Pools, é possível
      controlar o acesso a recursos compartilhado e garantir a consistência dos dados em ambientes de multithreading e
      paralelismo.
